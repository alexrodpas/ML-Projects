{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dab23bd-7b7f-4ff7-ba45-53d883018e60",
   "metadata": {},
   "source": [
    "# COVID 19 Detection from chest X-Rays using a CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efb472-21ac-41af-8c62-af36c0791be4",
   "metadata": {},
   "source": [
    "The COVID-19 pandemic has been, over the last 3 years, one of the biggest challenges for healthcare systems worldwide.\n",
    "\n",
    "In this notebook, we will use X-ray data of lungs from both normal and COVID-positive patients and train a deep learning model to differentiate between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc66f1-1118-44ee-8beb-b1c6290bd8d7",
   "metadata": {},
   "source": [
    "## Introduction - Dataset and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513b950-16fd-4736-875c-f3ea6d0bf971",
   "metadata": {},
   "source": [
    "The dataset used in this project is the Winner of the COVID-19 Dataset Award by Kaggle Community. The dataset was collected by researchers from Qatar and Bangladesh. It can be found at: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
    "\n",
    "References:\n",
    "-M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, M. T. Islam, “Can AI help in screening Viral and COVID-19 pneumonia?” IEEE Access, Vol. 8, 2020, pp. 132665 - 132676.[Paper link](https://ieeexplore.ieee.org/document/9144185)\n",
    "-Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Kashem, S.B.A., Islam, M.T., Maadeed, S.A., Zughaier, S.M., Khan, M.S. and Chowdhury, M.E., 2020. Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-ray Images. [Paper Link](https://doi.org/10.1016/j.compbiomed.2021.104319)\n",
    "\n",
    "This dataset contains a total of 21,215 images of 4 types:\n",
    "\n",
    "1. COVID-19 positive (3,616 images)\n",
    "1. Viral Pneumonia (1,395 images)\n",
    "1. Normal X-ray (10,192 images)\n",
    "1. Lung Opacity (6,012 images)\n",
    "\n",
    "We will only consider the first three types, and therefore we'll have to classify among these 3 different classes; we'll use a softmax layer for classification. These images have a size (1024, 1024) and 3 color channels. \n",
    "\n",
    "The authors of the dataset also trained a ResNet-34 model and achieved a classification accuracy of 98.5%. In this notebook we'll use the **Xception** model. Xception is a deep convolutional neural network (CNN) architecture that involves [Depthwise Separable Convolutions](https://paperswithcode.com/method/depthwise-separable-convolution). This network was introduced in the paper by Francois Chollet, [\"Xception: Deep Learning With Depthwise Separable Convolutions\"](https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html); Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1251-1258 . Xception is also known as “extreme” version of an Inception module. This model obtained an ImageNet top-1 accuracy of 79% and a top-5 accuracy of 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6ee8c-d641-4bf6-8231-42b857bd5ce7",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633a463-32e8-4e77-a706-7ff465b84aad",
   "metadata": {},
   "source": [
    "We will use and fine-tune a pre-trained version of Xception from the fantastic `timm` library ([link](https://github.com/huggingface/pytorch-image-models)). PyTorch Image Models (`timm`), a deep-learning library created by Ross Wightman, is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results. We will also use `fastai` ([link](https://docs.fast.ai/)), a deep learning library that builds on [*PyTorch*](https://pytorch.org/) and that helps simplify training neural networks by providing high-level components for standard deep learning tasks, `torchtnt`, a library for PyTorch training tools and utilities [(link)](https://pytorch.org/tnt/stable/), plus the usual libraries in the PyData stack: `numpy`, `pandas`, `sklearn` and `matplotlib`.\n",
    "\n",
    "### About fast.ai\n",
    "`fastai` is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7712c71-3dd2-44ef-8bce-c3e6a0e7a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install timm fastai torchtnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971dcbb-251d-45d4-a39f-78bab4180ae1",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06deaf6c-79d6-4590-a0c2-193bcaee1903",
   "metadata": {},
   "source": [
    "Now, we will import the necessary Python packages into our Jupyter Notebook. Here’s a brief overview of how we’ll use these packages:\n",
    "\n",
    "1. Python Standard Library dependencies: These are built-in modules that come with Python. We’ll use them for various tasks like handling file paths ([`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)), manipulating JSON files ([`json`](https://docs.python.org/3/library/json.html)), random number generation ([`random`](https://docs.python.org/3/library/random.html)), mathematical operations ([`math`](https://docs.python.org/3/library/math.html)), copying Python objects ([`copy`](https://docs.python.org/3/library/copy.html)), and working with dates and times ([`datetime`](https://docs.python.org/3/library/datetime.html)).\n",
    "1. Utility functions: These are helper functions from the packages we installed earlier. They provide shortcuts for routine tasks and keep our code clean and readable.\n",
    "1. matplotlib: We use the matplotlib package to explore the dataset samples and class distribution.\n",
    "1. NumPy: We’ll use it to store PIL Images as arrays of pixel values.\n",
    "1. pandas: We use Pandas `DataFrame` and `Series` objects to format data as tables.\n",
    "1. PIL (Pillow): We’ll use it for opening and working with image files.\n",
    "1. timm library: We’ll use the timm library to download and prepare a pre-trained Xception model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437cde14-09db-4031-9167-3f33797f0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library dependencies\n",
    "from copy import copy\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cff3a7-233f-4759-a270-9f776bcfc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy \n",
    "import numpy as np\n",
    "\n",
    "# Import pandas module for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Set options for Pandas DataFrame display\n",
    "pd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\n",
    "pd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n",
    "\n",
    "# Import PIL for image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Import timm library\n",
    "import timm\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "#torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "#from torchtnt.utils import get_module_summary\n",
    "\n",
    "# Import fastai computer vision functionality \n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b69ddd-bd96-4ca7-8092-b8685ea5abb1",
   "metadata": {},
   "source": [
    "### The timm library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b4b60-fe67-46dd-b91e-8125cb88a917",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the most popular features of `timm` is its large, and ever-growing collection of model architectures. Many of these models contain pretrained weights — either trained natively in PyTorch, or ported from other libraries such as Jax and TensorFlow — which can be easily downloaded and used.\n",
    "\n",
    "We can list, and query, the collection available models as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73299ac-f198-48a8-826d-7d9a50a1d814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timm.list_models('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4e3eb-ac4d-4a70-8615-173ce3acca0c",
   "metadata": {},
   "source": [
    "We can also use the `pretrained` argument to filter this selection to the models with pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03fe3e58-83ca-4471-af8f-72479b9ce13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timm.list_models(pretrained=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87e45c-c20c-4b7f-ab56-81209e96f3ee",
   "metadata": {},
   "source": [
    "## Setup project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbc3b0-7226-4125-90f1-02c55c131c60",
   "metadata": {},
   "source": [
    "In this section, we set up some basics for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756f1e5-41a9-40ec-af81-8ba2ea87e930",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "\n",
    "First, we set a seed for generating random numbers using the set_seed function included with the fastai library.\n",
    "A fixed seed value is helpful when training deep-learning models for reproducibility, debugging, and comparison.\n",
    "Having reproducible results allows others to confirm your findings. Using a fixed seed can make it easier to find bugs as it ensures the same inputs produce the same outputs. Likewise, using fixed seed values lets you compare performance between models and training parameters.\n",
    "That said, it’s often a good idea to test different seed values to see how your model’s performance varies between them. Also, don’t use a fixed seed value when you deploy the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39327d72-1ef7-463e-80b3-67cc9b1e174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for generating random numbers in PyTorch, NumPy, and Python's random module.\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31163a3-4d25-44d1-88b0-7e38a5897e32",
   "metadata": {},
   "source": [
    "### Set the PyTorch Device and Data Type\n",
    "Next, we determine the device to run our computations on and the data type of our tensors using fastai’s [`default_device()`](https://docs.fast.ai/torch_core.html#default_device) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a73c3e-1e86-4388-91c0-8fa24ee6ecad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = default_device()\n",
    "dtype = torch.float32\n",
    "device, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd622d-2eaa-4c7e-baad-37375a46740a",
   "metadata": {},
   "source": [
    "### Set directory paths\n",
    "\n",
    "We then need to set up a directory for our project to store our results and other related files. The following code creates the folder in the current directory (./). Update the path if that is not suitable for you.\n",
    "\n",
    "We also need a place to store our datasets and a location to download the zip file containing the dataset. Readers following the tutorial on their local machine should select locations with read and write access to store archive files and datasets. For a cloud service like Google Colab, you can set it to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2476d07c-efdc-4627-a476-302569d3fc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0b342\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0b342_level0_row0\" class=\"row_heading level0 row0\" >Project Directory:</th>\n",
       "      <td id=\"T_0b342_row0_col0\" class=\"data row0 col0\" >covid19-classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b342_level0_row1\" class=\"row_heading level0 row1\" >Dataset Directory:</th>\n",
       "      <td id=\"T_0b342_row1_col0\" class=\"data row1 col0\" >Datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b342_level0_row2\" class=\"row_heading level0 row2\" >Archive Directory:</th>\n",
       "      <td id=\"T_0b342_row2_col0\" class=\"data row2 col0\" >covid19-classifier/Archive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9a01ab9070>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A name for the project\n",
    "project_name = f\"covid19-classifier\"\n",
    "\n",
    "# The path for the project folder\n",
    "project_dir = Path(f\"./{project_name}/\")\n",
    "\n",
    "# Create the project directory if it does not already exist\n",
    "project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store datasets\n",
    "dataset_dir = Path(\"./Datasets/\")\n",
    "# Create the dataset directory if it does not exist\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define path to store archive files\n",
    "archive_dir = project_dir/'Archive/'\n",
    "# Create the archive directory if it does not exist\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.Series({\n",
    "    \"Project Directory:\": project_dir, \n",
    "    \"Dataset Directory:\": dataset_dir, \n",
    "    \"Archive Directory:\": archive_dir\n",
    "}).to_frame().style.hide(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d2c3a-4b0e-4309-9139-a8a3d9e00789",
   "metadata": {},
   "source": [
    "Double-check the project and dataset directories exist in the specified paths and that you can add files to them before continuing.\n",
    "\n",
    "At this point, our environment is set up and ready to go. We’ve set our random seed, determined our computation device, and set up directories for our project and dataset. In the next section, we will download and explore the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6680f54-6c0c-432a-99cd-775a69596530",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6535f5-c4fd-4a1a-8652-122b672a20cf",
   "metadata": {},
   "source": [
    "The following steps demonstrate how to download the dataset from Kaggle, inspect the dataset, and visualize some sample images. \n",
    "To download the Kaggle dataset to the local jupyter environment we will use the [`opendatasets`](https://pypi.org/project/opendatasets/) library, so before starting, we need to have the `opendatasets` library installed in our system. If it's not present in your system, use Python’s package manager pip and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0f5c3-9411-4c1d-80ee-bc7b2ace3b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install opendatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cac7b3-0053-4967-b8ff-b9bd4cbde43d",
   "metadata": {},
   "source": [
    "The process is as follows (**for your convenience I've already downloaded the Dataset, it's in the _Dataset/COVID19-Radiography-Dataset_ folder, so there's no need to execute the following cells, they are just to show how to use the opendatasets library.**)\n",
    "\n",
    "1. Import the opendatasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56ddc6-3893-415e-bdb6-61cd9167bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c348d6-65c5-403d-9135-4f153b18f766",
   "metadata": {},
   "source": [
    "2. Now we use the `download()` function of the `opendatasets` library, which as the name suggests, is used to download the dataset. It takes the link to the dataset as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163418b-14fd-4645-b11d-4a7c07718af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\"\n",
    "data_dir= \"Datasets\"\n",
    "od.download(url, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cb3a3-f4f7-42ce-9da6-8febfc49e03f",
   "metadata": {},
   "source": [
    "3. On executing the above line, it will prompt for your Kaggle username. Your Kaggle username can be fetched from the Account tab of the My Profile section.\n",
    "\n",
    "4. On entering your username, it will prompt for Kaggle Key. Again, go to the account tab of the My Profile section and click on Create New API Token. This will download a kaggle.json file.\n",
    "\n",
    "5. On opening this file, you will find the username and key in it. Copy the key and paste it into the prompted Jupyter Notebook cell. The content of the downloaded file would look like this:\n",
    "\n",
    "`{\"username\":<KAGGLE USERNAME>,\"key\":\"<KAGGLE KEY>\"}`\n",
    "\n",
    "6. Do NOT store the API Token (kaggle.json file) in your GitHub repository.\n",
    "\n",
    "7. A progress bar will show if the dataset is downloaded completely or not.\n",
    "\n",
    "8. After successful completion of the download, a folder will be created in the Datasets directory of your Jupyter Notebook. This folder contains our dataset.\n",
    "\n",
    "9. Since we will not be using any of the metadata info nor the Lung Opacity images, delete them. This will also make loading the dataset much easier (no need to filter out sub-directories when building the training and evaluation sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad303d-91ad-4626-8de4-ef66327c650e",
   "metadata": {},
   "source": [
    "### Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c79abd-52f8-4406-8462-a004b7796da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset path\n",
    "# To exclude the 'masks' subdirectories, we will create a new path that is the result of subtracting the excluded subdirectories \n",
    "# from the root Dataset directory. \n",
    "# The `rglob()` method is used to find all files and directories that match the specified glob pattern, \n",
    "# which in this case includes the exclude patterns. \n",
    "# The `-` operator is then used to subtract the excluded paths from the root directory, \n",
    "# resulting in a new path that does not include the excluded subdirectories.\n",
    "\n",
    "# Define the root directory and the files and subdirectories to exclude\n",
    "dataset_root_dir = Path(dataset_dir)\n",
    "\n",
    "# Define function that checks whether paths to files and subdirectories should be included in the dataset\n",
    "# This is defined by passing as argument the intended exclusion's pattern and type \n",
    "# (file extension or subdirectory name)\n",
    "def include_path(p, pattern, excl_type):\n",
    "    if excl_type == \"dir\":\n",
    "        out = [path for path in p.rglob(\"*\") if pattern not in path.stem]\n",
    "    elif excl_type == \"ext\":\n",
    "        out = [path for path in p.rglob(\"*\") if pattern not in path.suffix]\n",
    "    return(out)\n",
    "\n",
    "exclusions = [(\"masks\", \"dir\"),(\"*.xlsx\",\"ext\"),(\"*.txt\", \"ext\")]\n",
    "\n",
    "# Run function on the dataset root path with the intended exclusions\n",
    "dataset_path = list(chain(*[include_path(dataset_root_dir, *excl) for excl in exclusions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af417b16-897f-4e41-9f71-36165641c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file paths for each image in the dataset\n",
    "img_paths = get_image_files(dataset_path)\n",
    "\n",
    "# Get the number of samples for each image class\n",
    "class_counts = Counter(path.parent.name for path in img_paths)\n",
    "\n",
    "# Get the class names\n",
    "class_names = list(class_counts.keys())\n",
    "\n",
    "# Print the number of samples for each image class\n",
    "class_counts_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Count'])\n",
    "class_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62389c-7180-4b6f-be18-11186f837118",
   "metadata": {},
   "source": [
    "### Visualizing Sample Images\n",
    "\n",
    "Lastly, we will visualize a sample image from each class in our dataset. Visualizing the samples helps us get a feel for the kind of images we’re working with and whether they’re suitable for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfb80f-64a2-426d-bbcf-bcd7f5aa36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths for the first sample in the dataset for each class\n",
    "sample_paths = [next((dataset_path/class_name).iterdir()) for class_name in class_names]\n",
    "sample_paths.sort()\n",
    "\n",
    "# Calculate the number of rows and columns\n",
    "grid_size = math.floor(math.sqrt(len(sample_paths)))\n",
    "n_rows = grid_size+(1 if grid_size**2 < len(sample_paths) else 0)\n",
    "n_cols = grid_size\n",
    "\n",
    "# Create a list to store the first image found for each class\n",
    "images = [Image.open(path) for path in sample_paths]\n",
    "labels = [path.parent.name for path in sample_paths]\n",
    "\n",
    "# Create a figure for the grid\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # If we have an image for this subplot\n",
    "    if i < len(images) and images[i]:\n",
    "        # Add the image to the subplot\n",
    "        ax.imshow(np.array(images[i]))\n",
    "        # Set the title to the corresponding class name\n",
    "        ax.set_title(labels[i])\n",
    "        # Remove the axis\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        # If no image, hide the subplot\n",
    "        ax.axis('off')\n",
    "\n",
    "# Display the grid\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e885c-bdfe-422b-8309-1ba574814c66",
   "metadata": {},
   "source": [
    "The original Dataset is huge (over 20,000 images), so it would take a long time to train the model with the full dataset; for illustration purposes, we will do data augmentation, and therefore, we will take a random sample of 20% of images from each category. It also contains \"lung mask\" images (in the *masks* subdirectories of each of the directories for the 4 categories of images. We won't be using these, so we'll have to build a fastai `DataBlock` that skips them. First, let's define some parameters:\n",
    "\n",
    "1. we'll resize (if needed) the size of images to (299,299) (the maximum size accepted by the Xception model), \n",
    "1. we'll split the dataset into batches of size of 32,\n",
    "1. we'll use a traing/validation split of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eade547-f274-4d3b-bf19-230f44d06962",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "NUM_CHANNELS = 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
