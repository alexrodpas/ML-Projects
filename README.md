# ML-Projects
---
# Machine Learning projects for my own learning and training (pun intended).

A collection of Python files and scripts, and Jupyter notebooks across Classification, Regression, Natural Language Processing, Computer Vision and more.

1. **[TEDx talk (content-based) recommender system](https://github.com/alexrodpas/ML-Projects/blob/main/TED-Talks-RecSys.ipynb)**: In this notebook, we will build a very basic content-based Machine Learning recommender system that can recommend TED talks based on the topics of your interest.
1. **[COVID 19 Detection from chest X-Rays using a CNN Classifier](https://github.com/alexrodpas/ML-Projects/blob/main/COVID19-Dtct-XRay.ipynb)**: In this notebook, we will use X-ray data of lungs from normal, COVID-positive and viral-pneumonia patients, and train a deep learning model with a Convolutional Neural Network (CNN) architecture to differentiate between them. In particular, we will fine-tune the Xception model, a deep convolutional neural network (CNN) architecture that involves [Depthwise Separable Convolutions](https://paperswithcode.com/method/depthwise-separable-convolution). This network was introduced in the paper by Francois Chollet, ["Xception: Deep Learning With Depthwise Separable Convolutions"](https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html); Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1251-1258.
1. **[Abstractive Text Summarization with T5 Transformer Model](https://github.com/alexrodpas/ML-Projects/blob/main/ats_t5_01.ipynb)**: Automatic summarization is one of the central problems in Natural Language Processing (NLP). Summarization consists on creating a shorter version of a document or an article that captures all the important information. It poses several challenges relating to language understanding (e.g. identifying important content) and generation (e.g. aggregating and rewording the identified content into a summary). Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. Summarization can be *extractive* -extract the most relevant information from a document- or *abstractive* - generate new text that captures the most relevant information-. In this project we will approach the problem of single-document abstractive summarization. Following prior work, we aim to tackle this problem using the [T5 model](https://arxiv.org/abs/1910.10683). Text-to-Text Transfer Transformer (T5) is a [Transformer-based](https://arxiv.org/abs/1706.03762) model built on the encoder-decoder architecture, pretrained on a multi-task mixture of unsupervised and supervised tasks where each task is converted into a text-to-text format. We will fine-tune the pretrained T5 model on the Abstractive Summarization task using Hugging Face Transformers on the [Extreme Summarization (XSum)](https://arxiv.org/abs/1808.08745) dataset loaded from Hugging Face Datasets.
1. **[Extractive Question Answering with DistilBERT Transformer model](https://github.com/alexrodpas/ML-Projects/blob/main/extr-qa-distilBERT-01.ipynb)**. Question Answering (QA) is one of the central problems in Natural Language Processing (NLP). QA tasks return an answer given a question. Virtual assistants like Alexa, Siri or Google Assistant are examples of QA systems. There are two common types of QA tasks: **extractive** -extract the answer from the given context- and **abstractive** -generate an answer from the context that correctly answers the question. In this project we will fine-tune the DistilBERT Transformer model on the SQuAD dataset for extractive question answering., and then use this fine-tuned model for inference. The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/papers/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark. We will fine-tune the DistilBERT model on the extractive QA task using the Hugging Face Transformers library and the [SQuAD dataset](https://huggingface.co/datasets/squad). The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
